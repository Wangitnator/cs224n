{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem1intro1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem1intro2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem1a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Code](utils.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem1b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Code](model_embeddings.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem1c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Code](nmt_model.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem1d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[sanity_check.py](sanity_check.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem1e.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[sanity_check.py](sanity_check.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem1f.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[nmt_model.py](nmt_model.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem1g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "When we set $ enc_masks_i = 1$, the step() function sets $e_{t,i}$ to $-\\infty$, resulting in $\\alpha_{t,i} = 0$.\n",
    "\n",
    "This results in no attention being given to 'pad' tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem1h.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem1i.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer: (I forgot, but I think it was 24.17, or something like that)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem1j.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "1. Dot-Product Attention:\n",
    "   * Pro: Quick & easy to compute, especially for larger-dimensional tensors\n",
    "   * Con: doesn't allow for more complex interactions between  $\\mathbf{e}$ and $\\mathbf{s}$\n",
    "2. Multiplicative Attention:\n",
    "   * Pro: still (somewhat less) quick to compute.\n",
    "   * Con: model is constrained to assume linear dependence between $\\mathbf{e}$ and $\\mathbf{s}$.\n",
    "3. Addition Attention:\n",
    "   * Pro: more complex model, can better capture more complex relation ships between $\\mathbf{e}$ and $\\mathbf{s}$\n",
    "   * Con: three more parameters to train: $\\mathbf{v}$, $\\mathbf{W_{1}}$, and $\\mathbf{W_{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem2a_part1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem2a_part2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem2ai.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "1) NMT Errors\n",
    "\n",
    "**Error** $ \\rightarrow$  **Reference Translation**:\n",
    "\n",
    "\"Here's\" $ \\rightarrow$ \"So\"\n",
    "\n",
    "\"favorite\" $ \\rightarrow$ \"one\"\n",
    "\n",
    "2) NMT model can't generate pronouns (\"one\" in this example), even though it understands what antecedent is \"The Starry Night\".\n",
    "\n",
    "3) Add more training examples using pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem2aii.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "1) NMT Errors\n",
    "\n",
    "**Error** $ \\rightarrow$  **Reference Translation**:\n",
    "\n",
    "{no comma between \"know\" and \"what\" } $ \\rightarrow$ \"So\"\n",
    "\n",
    "\"author for children, more reading in the U.S.\" $ \\rightarrow$ \"America's most widely read children's author, in fact.\"\n",
    "\n",
    "2) NMT model seems to be doing more transliteration (word-by-word translation), rather than translating entire passage. It also has difficulty with conjunctive classes.\n",
    "\n",
    "3) Add more connections between earlier and later cells in encoder (skip connections?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem2aiii.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "1) NMT Errors\n",
    "\n",
    "**Error** $ \\rightarrow$  **Reference Translation**:\n",
    "\n",
    "\"Bolinbroke\" $ \\rightarrow$ \"&lt;unk&gt;\"\n",
    "\n",
    "2) Can't translate words it doesn't recognize, because it has no associated word embedding for them. In this instance the word is a not-so-common proper name.\n",
    "\n",
    "3) Incorporate sub-word modeling into our NMT model, to handle unrecognized words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem2aiv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "1) NMT Errors\n",
    "\n",
    "**Error** $ \\rightarrow$  **Reference Translation**:\n",
    "\n",
    "\"apple\" $ \\rightarrow$ \"block\"\n",
    "\n",
    "2) _Manzana_ has multiple noun senses: 1) apple, 2) block, and our NMT model recognizes the first (most common) one. Our word embeddings seem unable to learn the multiple senses that some words may have. Also, we might also presume that the training corpus contains more examples of \"apple\" _manzana_ usage within Spanish, rather than \"block\".\n",
    "\n",
    "3) We can use Contextualized Word Representations (e.g. BERT, ELMo), to capture semantic meaning of different senses of words, by creating word embeddings, based on context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem2av.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "1) NMT Errors\n",
    "\n",
    "**Error** $ \\rightarrow$  **Reference Translation**:\n",
    "\n",
    "\"woman's room\" $ \\rightarrow$ \"teachers' lounge\"\n",
    "\n",
    "2) It is likely that when decoder arrives at \"sala\", it places most attention to \"baño\", and thus translates \"sala\" to \"bathroom\". It may also do poorly translating phrases, and/or have few training examples with phrase \"sala de profesores\".\n",
    "\n",
    "3) Incorporate more examples of \"sala de profesores\" into training corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem2avi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "1) NMT Errors\n",
    "\n",
    "**Error** $ \\rightarrow$  **Reference Translation**:\n",
    "\n",
    "\"100,000 acres\" $ \\rightarrow$ \"250 thousand acres\"\n",
    "\n",
    "2) \"acres\" and \"hectare\" are semantically extremely similar. Their only difference in meaning is in actual units of land area. Our NMT model only recognizes semantic similarities, but cannot differentiate between different systems of unit measurement.\n",
    "\n",
    "3) Incorporate more examples of \"hectáreas\" translated as \"hectares\" into the training corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem2b1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "* **Example 1**: Tena un pulgar, y 85 dlares, y termin en San Francisco, California -- encontr un amante -- y en los aos '80, sent la necesidad de comenzar a trabajar en organizaciones que luchaban contra el SIDA.\n",
    "\n",
    "* **Example 2**: Casualmente, cuando Lorna y Judith trabajaban entre bastidores para reformar estos criterios, personas de todo el mundo vean un adulto autista por primera vez.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem2b2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "* **Example 1**: I had a thumb, I had 85 dollars,  and I ended up in San Francisco, California --  met a lover --  and back in the '80s, found it necessary  to begin work on AIDS organizations.\n",
    "\n",
    "* **Example 2**: By coincidence, as Lorna and Judith worked behind the scenes  to reform the criteria, people all over the world were seeing an autistic adult for the first time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem2b3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "* **Example 1:** I had a thumb, and I ended up in San Francisco, and it ended up in San Francisco, California -- I found a lover -- and in the years, I felt the need to start working in organizations that fight against AIDS.\n",
    "* **Example 2:** <unk> when I <unk> and Judith worked with me to reform these <unk> people all over the world to see a autistic child for the first time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem2b4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers:\n",
    "* **Example 1:** “I had 85 dollars” was missed from NMT translation\n",
    "* **Example 2:** <unk>'s appear where “Lorna” and “Coincidentally” should appear. Also, “I” and “me” spuriously appear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem2b5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers:\n",
    "* **Example 1:** can't handle long sentences with multiple clauses, and highly-stylized (but grammatically irregular) text.\n",
    "* **Example 2:** “Lorna” is not in ES or EN corpus, and “Coincidentally” is not in EN “corpus and/or “Casualmente” is not in ES corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem2b6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers:\n",
    "* **Example 1:** More training data? Deeper encoding network?\n",
    "* **Example 2:** More training data. Incorporate Char-CNN for recognizing new words. Add gazetteer for proper nouns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem2b3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem2ci.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "[Calculations](problem_2c_i.xls)\n",
    "\n",
    "Translation 2 has higher the BLEU score (0.63, vs. 0.54 for Translation 1), and reads as a better translation, to the human eye.\n",
    "\n",
    "In translation 1 \"always do\" can also mean \"always suffice\", which would be a semantically incorrect translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem2cii.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "[Calculations](problem_2c_ii.xls)\n",
    "\n",
    "Translation 1 has higher  the BLEU score (0.45, vs 0.26).\n",
    "\n",
    "But translation 2 remains the better translation, for reasons stated in part i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem2ciii.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers:\n",
    "Single translations are problematic, can encode stylistic bias of translator. \n",
    "\n",
    "There are usually many other, equally-correct translations.\n",
    "\n",
    "Relying on single reference translation, may result in our NMT models being incorrectly penalized for \"incorrect\" translations, that are otherwise ones a human reader would deem valid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/a4_problem2civ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "* **Advantages:**\n",
    "\n",
    "    1) Easy to compute\n",
    "    \n",
    "    2) Systematic, doesn't require human annotation, subjective judgment\n",
    "    \n",
    "    \n",
    "    \n",
    "* **Disadvantages**\n",
    "\n",
    "\t1) Doesn't always correspond to real-world results/readability\n",
    "    \n",
    "\t2) Favors verbiage, not overall semantic meaning.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
